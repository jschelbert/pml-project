---
title: "Classifying quality of weight lifting using sensor data"
subtitle: "Practical Machine Learning - Course Project"
author: "Jakob Schelbert"
date: "29.1.2017"
output: 
  html_document:
    keep_md: yes
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(plyr)
library(dplyr)
library(ggplot2)
library(caret)
library(knitr)
library(readr)
library(doMC)
registerDoMC(4)
seed <- 12345
set.seed(seed)
```

# Executive Summary
Fitness tracker or smart watches see an increasing popularity these days.
A promising application of the activity data collected by these devices is to give advice to the user how to improve their activity execution.
As a first step towards such an application we study a data set which consists of several participants executing dumbell lifting in a correct and in several wrong ways.
We aim to build a classifier that can distinguish between these different ways in a precise manner.
Random forrest and the C5.0 algorithm perform best on the data set and give an accuracy of 99%.


# Background Of Raw Data
We begin our analysis by loading the data.
Note that there are some special treatments necessary to load all columns as their correct type (especially column `datetime`).
```{r load_data, cache=TRUE, message=FALSE, warning=FALSE}
url1 <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
datafile1 <- "pml-training.csv"
if (!file.exists(datafile1)) {
  download.file(url1, datafile1)
}
url2 <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
datafile2 <- "pml-testing.csv"
if (!file.exists(datafile2)) {
  download.file(url2, datafile2)
}
training <- read_csv("pml-training.csv", col_types = cols(
  user_name = "c",
  raw_timestamp_part_1 = "d",
  raw_timestamp_part_2 = "d",
  cvtd_timestamp = col_datetime(format = "%d/%m/%Y %H:%M"),
  new_window = "c",
  classe = col_factor(c("A","B","C","D","E")),
  .default = col_double())
  )
testing <- read_csv("pml-testing.csv", col_types = cols(
  user_name = "c",
  raw_timestamp_part_1 = "d",
  raw_timestamp_part_2 = "d",
  cvtd_timestamp = col_datetime(format = "%d/%m/%Y %H:%M"),
  new_window = "c",
  problem_id = "i",
  .default = col_double())
  )
```
The data was originally created in the paper *Qualitative Activity Recognition of Weight Lifting Exercises* by Velloso, Bulling, Gellersen, Ugulino, and Fuks.
More information can be found on the corresponding [website](http://groupware.les.inf.puc-rio.br/har). 
The authors describe the process of obtaining the data set as the following:

> Participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: 
> exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). 
> Class A  corresponds  to  the  specified  execution  of  the  exercise, while  the  other  4  classes  correspond  to  common  mistakes.

# Exploratory Analysis
We start by examining the data.
The training data consists of nearly 20000 samples and has 160 variables.
The first columns consits of some information about the sample, e.g. the date the sample was obtained, the person performing the activity while the sample was taken etc.
There are 100 columns that predominantly contain NAs which stems from the fact that these colums aggregate data from other samples within a certain time window.
Thus, to have only columns without NAs we leave these columns out of our preprocessed training set.

```{r exploratory_analysis, cache=TRUE}
dim_train <- dim(training)
dim_test <- dim(testing)
train_condensed <- training %>% select(-contains("kurtosis_"),
                                       -contains("skewness_"),
                                       -contains("max_"),
                                       -contains("min_"),
                                       -contains("amplitude_"),
                                       -contains("var_"),
                                       -contains("avg_"),
                                       -contains("stddev_"))
ggplot(aes(x=classe, fill=user_name), data=train_condensed) + geom_bar() + labs(x="Activity class", fill="Participant")
```

We can see in the figure that activity class A has the most samples, while class D has the least.
For all participants the samples are consistently distributed among the different classes, therefore, we do not have to do some processing to handle different participants.

# Building A Predictor
For builing the predictor we first divide the training set further to be able to estimate an out-of-sample error.
```{r traindata_setup, cache=TRUE}
inTrain <- createDataPartition(train_condensed$classe, p = 3/4)[[1]]
training_train <- train_condensed[inTrain,]
training_test <-train_condensed[-inTrain,]
```
We will use the about 75% of the training set for our training (`training_train`) and the remaining 25% for measuring the performance and estimating the out-of-sample error (using `training_test`).

## Training the algorithm
For our model we tried several different algorithms.
Notice that the used `caret` package uses bootstrapping when given default parameters for training.
See the appendix section [Spot checking](#spot-checking) for some explanation and comparison of different algorithms.
The best performing algorithm was C5.0 (or see5) that is the successor of the famour C4.5 algorithm and uses decision trees to build a model.
For more information see [Wikipedia](https://en.wikipedia.org/wiki/C4.5_algorithm#Improvements_in_C5.0.2FSee5_algorithm) or the authors [homepage](http://www.rulequest.com/see5-unix.html).
```{r train_c50, cache=TRUE}
set.seed(seed)
model_c50 <- train(classe~., data = training_train[,8:60], method = "C5.0", importance = TRUE)
```

We also utilize the second best performing algorithm random forrest (`rf`).
```{r train_rf, cache=TRUE}
set.seed(seed)
model_rf <- train(classe~., data = training_train[,8:60], method = "rf", importance = TRUE)
```

It is interesting to also extract the importance of the variables.
If we want to build a small model (for example to fit in a smartwatch or fitness tracker) we could use this data to find the most promising variables to use in a small model.
```{r importance, cache=TRUE}
importance_rf <- data.frame(model_rf$finalModel$importance)
kable(importance_rf %>% tibble::rownames_to_column("Variable") %>% arrange(desc(MeanDecreaseAccuracy)) %>% head(10))
```

We can observe that under the top 10 variables most are associated with either the belt or the dumbell.
This could be a good indicator to further investigate these sensors or put increased emphasis on the development of the sensors.


## Evaluation
To evaluate the out-of-sample error we use our separated test set (`training_test`) to calculate predictions based on the fitted models.
A comparison with the correct activity classes reveals very promising accuracy of over 99% for both C5.0 and random forrest.
One outstanding result is that C5.0 can classify the correct activity execution (class A) without error.
This is important since correct execution of the activity is indeed classified as such.
```{r predict, cache=TRUE}
pred_rf <- predict(model_rf, newdata=training_test)
pred_c50 <- predict(model_c50, newdata=training_test)
confusionMatrix(pred_rf, reference = training_test$classe)
confusionMatrix(pred_c50, reference = training_test$classe)
```
More precisely we report a out-of-sample accuracy for C5.0 of `r confusionMatrix(pred_c50, reference = training_test$classe)$overall[1]` (or out-of-sample error of `r 1-confusionMatrix(pred_c50, reference = training_test$classe)$overall[1]`.
Likewise the out-of-sample accuracy for random forrest is `r confusionMatrix(pred_rf, reference = training_test$classe)$overall[1]` (or out-of-sample error of `r 1-confusionMatrix(pred_rf, reference = training_test$classe)$overall[1]`.


## Conclusion
We trained two models with data from activity sensors.
Both C5.0 as well as random forrests perform very good and deliver an accuracy of over 99%, thus, a out-of-sample error under 1%.
Further room for improvement could be to evaluate the importance of the variables to build a light-weight prediction model.
Also, a classification that has an even higher specificity for class A (correct activity execution) could help to detect wrong execution of the user.


# Appendix
In the appendix we state the spot-checking code that compares several differnt algorithms to get a good starting point which methods should be investigated further.
We also include all the used code and some info on the package versions.
Note that the original github repository of this project can be found [here](https://github.com/jschelbert/pml-project).

## Spot-Checking
In order to find the best algorithm for our prediction problem we tested several common packages that can be efficiently used within the `caret` package.
More precisely, we tested
* Linear discriminant analysis (method `lda`)
* Logistic regression (method `glm`)
* Neural net (method `glmnet`)
* Support vector machine - SVM (method `svmRadial`)
* K-nearest-neigbour (method `knn`)
* C5.0 (method `c50`)
* CART (method `cart`)
* Naive Bayes (method `nb`)
* Random forrest (methods `rf` and `Rborist`)
* Bagged CART (method `treebag`)
```{r train_algos, cache=TRUE}
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "Accuracy"

# Linear Discriminant Analysis
set.seed(seed)
fit.lda <- train(classe~., data = training_train[,8:60], method="lda", metric=metric, preProc=c("center", "scale"), trControl=control)
# GLMNET
set.seed(seed)
fit.glmnet <- train(classe~., data = training_train[,8:60], method="glmnet", metric=metric, preProc=c("center", "scale"), trControl=control)
# SVM Radial
set.seed(seed)
fit.svmRadial <- train(classe~., data = training_train[,8:60], method="svmRadial", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE)
# kNN
set.seed(seed)
fit.knn <- train(classe~., data = training_train[,8:60], method="knn", metric=metric, preProc=c("center", "scale"), trControl=control)
# Naive Bayes
set.seed(seed)
fit.nb <- train(classe~., data = training_train[,8:60], method="nb", metric=metric, trControl=control)
# CART
set.seed(seed)
fit.cart <- train(classe~., data = training_train[,8:60], method="rpart", metric=metric, trControl=control)
# C5.0
set.seed(seed)
fit.c50 <- train(classe~., data = training_train[,8:60], method="C5.0", metric=metric, trControl=control)
# Bagged CART
set.seed(seed)
fit.treebag <- train(classe~., data = training_train[,8:60], method="treebag", metric=metric, trControl=control)
# Random Forest
set.seed(seed)
fit.rf <- train(classe~., data = training_train[,8:60], method="rf", metric=metric, trControl=control)
# Stochastic Gradient Boosting (Generalized Boosted Modeling)
set.seed(seed)
fit.gbm <- train(classe~., data = training_train[,8:60], method="gbm", metric=metric, trControl=control, verbose=FALSE)
# Another random forrest implementation
set.seed(seed)
fit.Rborist <- train(classe~., data = training_train[,8:60], method = "Rborist", metric=metric, trControl=control)
```
Using the resulting models we can utilize the `resamples` function to show an overview of the accuracy and kappa of each model.
It is clear that random forrest, C5.0 and bagged CART (`treebag`) give very good results.
```{r algos_results, cache=TRUE}
results <- resamples(list(lda=fit.lda, glmnet=fit.glmnet,
	svmRadial=fit.svmRadial, knn=fit.knn, nb=fit.nb, cart=fit.cart, c50=fit.c50,
	treebag=fit.treebag, rf=fit.rf, gbm=fit.gbm, Rborist=fit.Rborist), decreasing = TRUE)
# Table comparison
kable(summary(results)$statistics$Accuracy)
```

A graphical representation of the results is given by the following boxplot.
```{r bwplot, cache=TRUE}
bwplot(results)
```

## Copyright
This report is released unter **CC BY-SA**.

[![](by-sa.png)](https://creativecommons.org/licenses/by-sa/3.0/)

## Code
```{r, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, tidy=TRUE}
```

## Used packages and information on computer
```{r}
sessionInfo()
```

